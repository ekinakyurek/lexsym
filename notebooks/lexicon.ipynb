{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "def load_lexicon(lexicon_path, train_path):\n",
    "    lexicon = json.load(open(lexicon_path))\n",
    "    inputs = []\n",
    "    with open(train_path, 'r') as f:\n",
    "        for line in f:\n",
    "            inputs.append(line.split('\\t')[:2])\n",
    "    return lexicon, inputs\n",
    "\n",
    "def filter_uncommon_tokens(lexicon, threshold):\n",
    "    # Filter uncommon tokens\n",
    "    deleted_keys = set()\n",
    "    \n",
    "    for (k1, v1) in lexicon.items():\n",
    "        deleted_codes = set()\n",
    "        \n",
    "        for c, count in v1.items():\n",
    "            if count < threshold:\n",
    "                deleted_codes.add(c)\n",
    "        \n",
    "        for k in deleted_codes:\n",
    "            del v1[k]\n",
    "            \n",
    "        if len(v1) == 0:\n",
    "            deleted_keys.add(k1)\n",
    "            \n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "        \n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def filter_intersected_tokens(lexicon):\n",
    "    deleted_keys = set()\n",
    "    for (k1, v1) in lexicon.items():\n",
    "        for ci, count in v1.items():\n",
    "            for (k2, v2) in lexicon.items():\n",
    "                if k2 == k1:\n",
    "                    continue\n",
    "                if ci in v2:\n",
    "                    deleted_keys.add(k1)\n",
    "                    deleted_keys.add(k2)\n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "    return lexicon\n",
    "    \n",
    "\n",
    "def get_swapables(lexicon, inputs):\n",
    "    inputs = copy.deepcopy(inputs)\n",
    "    random.shuffle(inputs)\n",
    "    swapables = {k: [] for k in lexicon.keys()}\n",
    "    for k1 in lexicon.keys():\n",
    "        for k2 in lexicon.keys():\n",
    "            if k1 != k2:\n",
    "                if k1 in swapables[k2]:\n",
    "                    swapables[k1].append(k2)\n",
    "                else:   \n",
    "                    x1s = itertools.islice(filter(lambda x: k1 in x, inputs), 5000)\n",
    "                    x2s = itertools.islice(filter(lambda x: k2 in x, inputs), 5000)\n",
    "                    for (x1, x2) in itertools.product(x1s, x2s):\n",
    "                        if ' ' in x1 and ' ' in x2 and x1.replace(k1, k2) == x2:\n",
    "                            swapables[k1].append(k2)\n",
    "                            print(f\"Linked {k1} - {k2}\")\n",
    "                            break\n",
    "    deleted_keys = set()               \n",
    "    for k, v in swapables.items():\n",
    "        if len(v) == 0:\n",
    "            deleted_keys.add(k)\n",
    "            \n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "        del swapables[k]\n",
    "             \n",
    "    return (lexicon, swapables)\n",
    "\n",
    "def propagate_swaps(swapables):\n",
    "    \n",
    "    for k1, swaps in swapables.items():\n",
    "        for k2 in swaps:\n",
    "            swaps2 = swapables[k2]\n",
    "            if k1 in swaps2 and k2 not in swaps:\n",
    "                swaps.append(k2)\n",
    "            elif k2 in swaps and k1 not in swaps2:\n",
    "                swaps2.append(k1)\n",
    "    \n",
    "    for k1, swaps in swapables.items():\n",
    "        for k2 in swaps:\n",
    "            for k3 in swapables[k2]:\n",
    "                if k3 != k2 and k3 not in swaps:\n",
    "                    swaps.append(k3)\n",
    "\n",
    "    return swapables\n",
    "    \n",
    "  \n",
    "def filter_lexicon_v2(lexicon, inputs):\n",
    "    lexicon = copy.deepcopy(lexicon)\n",
    "    lexicon = filter_uncommon_tokens(lexicon, 0) # len(inputs)/100)\n",
    "    lexicon = filter_intersected_tokens(lexicon)\n",
    "    lexicon, swapables = get_swapables(lexicon, inputs)\n",
    "    return lexicon, propagate_swaps(swapables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon, inputs = load_lexicon(\"/raid/lingo/akyurek/git/align/COGS/cogs/alignments/intersect.align.o.json\", \"/raid/lingo/akyurek/git/align/COGS/cogs/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lexicon, swapables = filter_lexicon_v2(lexicon, [input[0] for input in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swapables['baked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/raid/lingo/akyurek/git/align/COGS/cogs/alignments/lexicon_and_swapables_v2.json\",\"w\") as f:\n",
    "    json.dump({'lexicon': filtered_lexicon, 'swapables': swapables}, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open(\"/raid/lingo/akyurek/git/align/COGS/cogs/alignments/lexicon_and_swapables.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "lex = json.load(open(\"/afs/csail.mit.edu/u/a/akyurek/akyurek/git/align/TRANSLATE/alignments/intersect.align.o-swaps.jl.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in lex[\"swapables\"].items():\n",
    "    lex[\"swapables\"][k] = np.unique(v).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = [], []\n",
    "with open(\"/afs/csail.mit.edu/u/a/akyurek/akyurek/git/align/TRANSLATE/cmn.txt_train_tokenized.tsv.fast\") as f:\n",
    "    for d in f:\n",
    "        input, output = d.split(' ||| ')\n",
    "        inputs.append(input.strip().split(' '))\n",
    "        outputs.append(output.strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_outputs = [], []\n",
    "with open(\"/afs/csail.mit.edu/u/a/akyurek/akyurek/git/align/TRANSLATE/cmn.txt_test_tokenized.tsv\") as f:\n",
    "    for d in f:\n",
    "        d = d.replace(\"@@ \", \"\")\n",
    "        input, output = d.split('\\t')\n",
    "        test_inputs.append(input.strip())\n",
    "        test_outputs.append(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy.ma as ma\n",
    "import random \n",
    "import numpy as np\n",
    "MASK=\"UNK\"\n",
    "\n",
    "def masked_fill_(array, mask, value):\n",
    "    for i, m in enumerate(mask):\n",
    "        if m:\n",
    "            array[i]=value\n",
    "    \n",
    "def swap_ids(tensor, id1, id2, substitute=False):\n",
    "    if substitute:\n",
    "        masked_fill_(tensor, tensor == id1, id2)\n",
    "    else:\n",
    "        masked_fill_(tensor, tensor == id1, MASK)\n",
    "        masked_fill_(tensor, tensor == id2, id1)\n",
    "        masked_fill_(tensor, tensor == MASK, id2)\n",
    "\n",
    "    \n",
    "def make_a_swap_single(inp, out, lex_and_swaps, steps=0, substitute=False):\n",
    "    lexicon, swapables = lex_and_swaps['lexicon'], lex_and_swaps['swapables']\n",
    "    \n",
    "    keys = list(filter(lambda k: k in inp, lexicon.keys()))\n",
    "    \n",
    "    ## Add substitute\n",
    "    \n",
    "    if len(keys) != 0:\n",
    "        k1 = random.choice(keys)\n",
    "        weights=[1 / next(iter(lexicon[k].values())) for k in swapables[k1]]\n",
    "        k2 = random.choices(swapables[k1], weights=weights, k=1)[0]\n",
    "        ks = [k1, k2]\n",
    "    else:\n",
    "        k1 = random.choice(list(lexicon.keys()))\n",
    "        weights =  [1 / next(iter(lexicon[k].values())) for k in swapables[k1]]\n",
    "        k2 = random.choices(swapables[k1], weights=weights, k=1)[0]\n",
    "        ks = [k1, k2]\n",
    "        \n",
    "    #print(ks)\n",
    "    ks_q_id = ks    \n",
    "    swap_ids(inp, *ks_q_id, substitute=substitute)\n",
    "    \n",
    "    # print(lexicon[ks[0]])\n",
    "    # print(lexicon[ks[1]])\n",
    "    if substitute:\n",
    "        for v, _ in lexicon[ks[0]].items():\n",
    "            code2 = random.choice(list(lexicon[ks[1]].keys()))\n",
    "            masked_fill_(out, out == v, code2)\n",
    "    else:\n",
    "        for v, _ in lexicon[ks[0]].items():\n",
    "            masked_fill_(out, out == v, MASK)\n",
    "\n",
    "        for v, _ in lexicon[ks[1]].items():\n",
    "            code1 = random.choice(list(lexicon[ks[0]].keys()))\n",
    "            masked_fill_(out, out == v, code1)\n",
    "\n",
    "        code2 = random.choice(list(lexicon[ks[1]].keys()))\n",
    "\n",
    "        masked_fill_(out, out == MASK, code2)\n",
    "        # print(\"out swap: \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = set(test_inputs)\n",
    "train = set([\" \".join(i) for i in inputs])\n",
    "\n",
    "for i in range(10):\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        aug_inp = np.array(inp.copy(), dtype=object)\n",
    "        aug_out = np.array(out.copy(),  dtype=object)\n",
    "        make_a_swap_single(aug_inp, aug_out, lex)\n",
    "        make_a_swap_single(aug_inp, aug_out, lex)\n",
    "        aug_inp = \" \".join(aug_inp)\n",
    "        aug_out = \" \".join(aug_out)\n",
    "        if aug_inp  in test:\n",
    "            print(aug_inp)\n",
    "            print(\"pred: \", aug_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog at the store used the entire bottle .'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array(['泰勒', '决定', '继续', '用', '纸质', '书', '。'], dtype=object) \n",
    "v = '泰勒'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out == v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['泰勒', '决定', '继续', '用', '纸质', '书', '。'], dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    " masked_fill_(out, out == v, MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UNK', '决定', '继续', '用', '纸质', '书', '。'], dtype=object)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69a108bae46af32d512f4775e0c4070a0bdaed546c7c2aa4112cfa9703e4ad8a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('generative': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
