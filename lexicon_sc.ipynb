{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "\n",
    "def filter_lexicon(lexicon):\n",
    "    keys_to_hold = \"yellow,red,green,cyan,purple,blue,gray,brown\".split(\",\")\n",
    "    deleted_keys = set()\n",
    "    for k in lexicon.keys():\n",
    "        if k not in keys_to_hold:\n",
    "            deleted_keys.add(k)\n",
    "\n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def load_lexicon(lexicon_path, train_path):\n",
    "    lexicon = json.load(open(lexicon_path))\n",
    "    inputs = []\n",
    "    with open(train_path, 'r') as f:\n",
    "        for line in f:\n",
    "            inputs.append(line.split('\\t')[0])\n",
    "    return lexicon, inputs\n",
    "\n",
    "def filter_uncommon_tokens(lexicon, threshold):\n",
    "    # Filter uncommon tokens\n",
    "    deleted_keys = set()\n",
    "    \n",
    "    for (k1, v1) in lexicon.items():\n",
    "        deleted_codes = set()\n",
    "        \n",
    "        for c, count in v1.items():\n",
    "            if count < threshold:\n",
    "                deleted_codes.add(c)\n",
    "        \n",
    "        for k in deleted_codes:\n",
    "            del v1[k]\n",
    "            \n",
    "        if len(v1) == 0:\n",
    "            deleted_keys.add(k1)\n",
    "            \n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "        \n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def filter_intersected_tokens(lexicon):\n",
    "    deleted_keys = set()\n",
    "    for (k1, v1) in lexicon.items():\n",
    "        for ci, count in v1.items():\n",
    "            for (k2, v2) in lexicon.items():\n",
    "                if k2 == k1:\n",
    "                    continue\n",
    "                if ci in v2:\n",
    "                    deleted_keys.add(k1)\n",
    "                    deleted_keys.add(k2)\n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "    return lexicon\n",
    "    \n",
    "\n",
    "def get_swapables(lexicon, inputs):\n",
    "    inputs = copy.deepcopy(inputs)\n",
    "    random.shuffle(inputs)\n",
    "    swapables = {k: [] for k in lexicon.keys()}\n",
    "    for k1 in lexicon.keys():\n",
    "        for k2 in lexicon.keys():\n",
    "            if k1 != k2:\n",
    "                if k1 in swapables[k2]:\n",
    "                    swapables[k1].append(k2)\n",
    "                else:   \n",
    "                    x1s = itertools.islice(filter(lambda x: k1 in x, inputs), 5000)\n",
    "                    x2s = itertools.islice(filter(lambda x: k2 in x, inputs), 5000)\n",
    "                    for (x1, x2) in itertools.product(x1s, x2s):\n",
    "                        if x1.replace(k1, k2) == x2:\n",
    "                            swapables[k1].append(k2)\n",
    "                            print(f\"Linked {k1} - {k2}\")\n",
    "                            break\n",
    "    deleted_keys = set()               \n",
    "    for k, v in swapables.items():\n",
    "        if len(v) == 0:\n",
    "            deleted_keys.add(k)\n",
    "            \n",
    "    for k in deleted_keys:\n",
    "        del lexicon[k]\n",
    "        del swapables[k]\n",
    "             \n",
    "    return (lexicon, swapables)\n",
    "\n",
    "def propagate_swaps(swapables): \n",
    "    for k1, swaps in swapables.items():\n",
    "        for k2 in swaps:\n",
    "            swaps2 = swapables[k2]\n",
    "            if k1 in swaps2 and k2 not in swaps:\n",
    "                swaps.append(k2)\n",
    "            elif k2 in swaps and k1 not in swaps2:\n",
    "                swaps2.append(k1)\n",
    "    return swapables\n",
    "    \n",
    "  \n",
    "def filter_lexicon_v2(lexicon, inputs):\n",
    "    lexicon = copy.deepcopy(lexicon)\n",
    "    lexicon = filter_uncommon_tokens(lexicon, len(inputs)/100)\n",
    "    lexicon = filter_intersected_tokens(lexicon)\n",
    "    lexicon, swapables = get_swapables(lexicon, inputs)\n",
    "    return lexicon, propagate_swaps(swapables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Pdb\n",
    "#this one triggers the debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked cyan - red\n",
      "Linked cyan - gray\n",
      "Linked cyan - blue\n",
      "Linked cyan - purple\n",
      "Linked cyan - green\n",
      "Linked cyan - yellow\n",
      "Linked cyan - big\n",
      "Linked cyan - brown\n",
      "Linked red - gray\n",
      "Linked red - blue\n",
      "Linked red - purple\n",
      "Linked red - green\n",
      "Linked red - yellow\n",
      "Linked red - big\n",
      "Linked red - brown\n",
      "Linked gray - blue\n",
      "Linked gray - purple\n",
      "Linked gray - green\n",
      "Linked gray - yellow\n",
      "Linked gray - big\n",
      "Linked gray - brown\n",
      "Linked blue - purple\n",
      "Linked blue - green\n",
      "Linked blue - yellow\n",
      "Linked blue - big\n",
      "Linked blue - brown\n",
      "Linked purple - green\n",
      "Linked purple - yellow\n",
      "Linked purple - big\n",
      "Linked purple - brown\n",
      "Linked green - yellow\n",
      "Linked green - big\n",
      "Linked green - brown\n",
      "Linked yellow - big\n",
      "Linked yellow - brown\n",
      "Linked big - brown\n",
      "{'cyan': ['red', 'gray', 'blue', 'purple', 'green', 'yellow', 'big', 'brown'], 'red': ['cyan', 'gray', 'blue', 'purple', 'green', 'yellow', 'big', 'brown'], 'gray': ['cyan', 'red', 'blue', 'purple', 'green', 'yellow', 'big', 'brown'], 'blue': ['cyan', 'red', 'gray', 'purple', 'green', 'yellow', 'big', 'brown'], 'purple': ['cyan', 'red', 'gray', 'blue', 'green', 'yellow', 'big', 'brown'], 'green': ['cyan', 'red', 'gray', 'blue', 'purple', 'yellow', 'big', 'brown'], 'yellow': ['cyan', 'red', 'gray', 'blue', 'purple', 'green', 'big', 'brown'], 'big': ['cyan', 'red', 'gray', 'blue', 'purple', 'green', 'yellow', 'brown'], 'brown': ['cyan', 'red', 'gray', 'blue', 'purple', 'green', 'yellow', 'big']}\n"
     ]
    }
   ],
   "source": [
    "for clevr_type in (\"clevr\",):\n",
    "    for seed in range(3, 4):\n",
    "        exp_root = f\"clip_exp_img_seed_{seed}_{clevr_type}/clevr/VQVAE/beta_1.0_ncodes_32_ldim_64_dim_128_lr_0.0003/\"\n",
    "        lexicon, inputs = load_lexicon(exp_root + \"diag.align.o.json\", exp_root + \"train_encodings.txt\")\n",
    "        filtered_lexicon, swapables = filter_lexicon_v2(lexicon, inputs)\n",
    "        print(swapables)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69a108bae46af32d512f4775e0c4070a0bdaed546c7c2aa4112cfa9703e4ad8a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('generative': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
